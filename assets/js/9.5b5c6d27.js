(window.webpackJsonp=window.webpackJsonp||[]).push([[9],{217:function(e,t,a){"use strict";a.r(t);var s=a(0),r=Object(s.a)({},(function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("h1",{attrs:{id:"kubernetes-deployments"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#kubernetes-deployments","aria-hidden":"true"}},[e._v("#")]),e._v(" Kubernetes deployments")]),e._v(" "),a("h2",{attrs:{id:"kubectl"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#kubectl","aria-hidden":"true"}},[e._v("#")]),e._v(" Kubectl")]),e._v(" "),a("p",[a("a",{attrs:{href:"https://kubernetes.io/docs/tasks/tools/install-kubectl/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Install and Set Up kubectl - Kubernetes"),a("OutboundLink")],1)]),e._v(" "),a("p",[e._v("Kubectl connects to the cluster using the token or certificate stored in the kubectl config file. By default this is found at ~/.kube/config")]),e._v(" "),a("p",[e._v("When dealing with multiple clusters it’s possible to have multiple kubeconfig files defined in an environment variable KUBECONFIG. Files can be separated by a colon. Make sure you include the default kubeconfig file.")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("KUBECONFIG=~/.kube/config:~/my-k8s-cluster/kubeconfig.yaml:~/my-other-k8s-cluster/kubeconfig.yaml\n")])])]),a("p",[e._v("To get the contents of the kubeconfig file Browse to the Rancher cluster menu for your chosen cluster. Select “Cluster” from the horizontal menu and then click the “Kubeconfig file” button. Copy the contents into your local kubeconfig file and add to the KUBECONFIG env var.")]),e._v(" "),a("h3",{attrs:{id:"switching-between-clusters"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#switching-between-clusters","aria-hidden":"true"}},[e._v("#")]),e._v(" Switching between clusters")]),e._v(" "),a("p",[e._v("Each config file will have one or more named contexts defined. You can switch between clusters by using:")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v('shell> kubectl config use-context my-k8s-cluster\nSwitched to context "my-k8s-cluster".\n')])])]),a("p",[e._v("Find the current cluster context with")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("shell> kubectl config current-context\nmy-k8s-cluster\n")])])]),a("p",[e._v("and cluster info with:")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("shell> kubectl cluster-info\nKubernetes master is running at https://rancher2-staging.croud.tech/k8s/clusters/c-jmfl8\nKubeDNS is running at https://rancher2-staging.croud.tech/k8s/clusters/c-jmfl8/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n")])])]),a("h3",{attrs:{id:"visual-studio-code-plugin"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#visual-studio-code-plugin","aria-hidden":"true"}},[e._v("#")]),e._v(" Visual Studio Code plugin")]),e._v(" "),a("p",[e._v("Cluster management is a little easier if you use the Kubernetes Visual Studio Code plugin. (ms-kubernetes-tools.vscode-kubernetes-tools).")]),e._v(" "),a("p",[e._v("This will create an icon in the VS Code Toolbar that allows you to easily switch between namespaces and view kubernetes resources and config.")]),e._v(" "),a("h2",{attrs:{id:"helm"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#helm","aria-hidden":"true"}},[e._v("#")]),e._v(" Helm")]),e._v(" "),a("p",[a("a",{attrs:{href:"https://github.com/helm/helm#install",target:"_blank",rel:"noopener noreferrer"}},[e._v("Installing Helm"),a("OutboundLink")],1)]),e._v(" "),a("p",[e._v("Helm allows you to manage kunernetes api resources using “Helm Charts”.")]),e._v(" "),a("h3",{attrs:{id:"helm-chart"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#helm-chart","aria-hidden":"true"}},[e._v("#")]),e._v(" Helm Chart")]),e._v(" "),a("p",[e._v("A Helm Chart is a collection of api resource definition templates (using the GoLang templating syntax) with a set of default values for creating a release.")]),e._v(" "),a("h3",{attrs:{id:"helm-release"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#helm-release","aria-hidden":"true"}},[e._v("#")]),e._v(" Helm release")]),e._v(" "),a("p",[e._v("Releases are created when installing a helm chart to a cluster. Helm chart releases on a cluster can be listed with "),a("code",[e._v("helm ls")]),e._v(".")]),e._v(" "),a("p",[e._v("Create a release with:")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("helm upgrade --install my-unique-release-name stable/my-chart --namespace my-namespace --values=./some-common-values.yaml --values=./some-more-specific-values.yaml\n")])])]),a("h3",{attrs:{id:"helm-values"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#helm-values","aria-hidden":"true"}},[e._v("#")]),e._v(" Helm values")]),e._v(" "),a("p",[e._v("Helm values tend to follow a specific pattern. Viewing the chart default values should give you enough information to create a custom values file for your release. Values are inherited so you only need to include the values in your release file that you want to change. It’s also possible to add multiple values files which can help prevent repetition by having values files that might be common to all releases (DB Hosts, SMTP creds for example).")]),e._v(" "),a("h3",{attrs:{id:"creating-helm-charts"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#creating-helm-charts","aria-hidden":"true"}},[e._v("#")]),e._v(" Creating helm charts")]),e._v(" "),a("p",[e._v("To create a helm chart use the "),a("code",[e._v("helm create my-chart")]),e._v(" command. This will create a chart using the boilerplate helm chart files so all you need to do is modify it. It’s important to follow the conventions used in the boilerplate files (pod naming etc).")]),e._v(" "),a("h3",{attrs:{id:"publishing-helm-charts"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#publishing-helm-charts","aria-hidden":"true"}},[e._v("#")]),e._v(" Publishing helm charts")]),e._v(" "),a("p",[e._v("In addition to the official stable and incubator chart repositories it’s possible to publish our own charts to a repository we host in AWS S3. We have two S3 hosted repositories, "),a("em",[e._v("croudtech-stable")]),e._v(" and "),a("em",[e._v("croudtech-incubator")]),e._v(". Charts in ‘stable’ are considered production ready. Chart’s in incubator are considered bleeding edge and may not work.")]),e._v(" "),a("p",[e._v("To publish a chart to our repo simply create the chart in the ‘stable’ directory of https://github.com/CroudTech/devops-helm-charts and push to GitHub. Pushes to the master branch will publish to the ‘stable’ repository and pushes to the ‘incubator’ branch will publish to the incubator repository. Incubator charts will publish on every push. Stable charts need the version number bumping in the charts Chart.yaml file.")]),e._v(" "),a("h3",{attrs:{id:"using-the-custom-helm-repositories"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#using-the-custom-helm-repositories","aria-hidden":"true"}},[e._v("#")]),e._v(" Using the custom helm repositories")]),e._v(" "),a("p",[e._v("To use a custom repository you must add it to your local helm installation.")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v('shell> helm repo add croudtech-incubator s3://croudtech-helm-charts/croudtech-incubator\n"croudtech-incubator" has been added to your repositories\nshell> helm repo add croudtech-stable s3://croudtech-helm-charts/croudtech-stable\n"croudtech-stable" has been added to your repositories\n')])])]),a("p",[e._v("To update a chart (when new charts or chart versions have been published)")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v('shell> helm repo update\nHang tight while we grab the latest from your chart repositories...\n...Skip local chart repository\n...Successfully got an update from the "incubator" chart repository\n...Successfully got an update from the "croudtech-incubator" chart repository\n...Successfully got an update from the "croudtech-stable" chart repository\n...Successfully got an update from the "stable" chart repository\nUpdate Complete. ⎈ Happy Helming!⎈\n')])])]),a("h2",{attrs:{id:"autohelm"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#autohelm","aria-hidden":"true"}},[e._v("#")]),e._v(" Autohelm")]),e._v(" "),a("p",[e._v("Autohelm is a wrapper for Helm. It allows multiple chart releases to be specified in a single file.")]),e._v(" "),a("h3",{attrs:{id:"prerequisites"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#prerequisites","aria-hidden":"true"}},[e._v("#")]),e._v(" Prerequisites")]),e._v(" "),a("ul",[a("li",[a("p",[e._v("Python2 Runtime")]),e._v(" "),a("p",[a("em",[e._v("Autohelm iself is built on Python2 and as such will require the Python2 runtime present")])])]),e._v(" "),a("li",[a("p",[e._v("Helm")])]),e._v(" "),a("li",[a("p",[e._v("Autohelm")])]),e._v(" "),a("li",[a("p",[e._v("Autohelm S3 Plugin")]),e._v(" "),a("p",[a("em",[e._v("The Croud Helm Chart repository is stored in S3")])])])]),e._v(" "),a("h3",{attrs:{id:"overview"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#overview","aria-hidden":"true"}},[e._v("#")]),e._v(" Overview")]),e._v(" "),a("p",[e._v("An auto helm file represents a single namespace.  And values can be added in-line or by referencing values files.")]),e._v(" "),a("p",[e._v("For example:")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("namespace: kong\nrepositories:\n  croudtech-incubator:\n    url: s3://croudtech-helm-charts/croudtech-incubator\n  croudtech-stable:\n    url: s3://croudtech-helm-charts/croudtech-stable\nminimum_versions: #set minimum version requirements here\n  helm: 2.10.0\n  autohelm: 0.6.5\ncharts:\n  ingress-kong:\n    chart: kong-ingress\n    repository: croudtech-stable\n    version: 0.1.3\n    files:\n      - ../values/kong-ingress/kong-ingress.yaml\n  kong-dashboard:\n    chart: konga\n    repository: croudtech-stable\n    version: 0.1.3\n    files:\n      - ../values/konga/konga.yaml\n")])])]),a("p",[e._v("To deploy the charts in this auto helm file run:")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("autohelm plot ./my-autohelm-file.yaml\n")])])]),a("p",[e._v("To deploy only a specific chart defined in the file run:")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("autohelm plot ./my-autohelm-file.yaml --only kong-dashboard\n")])])]),a("p",[e._v("Any chart repositories defined in the auto helm file will be automatically added or updated so you won’t need to use the "),a("code",[e._v("helm repo *")]),e._v(" commands mentioned in the helm section fo this document.")]),e._v(" "),a("h2",{attrs:{id:"autohelm-workflow"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#autohelm-workflow","aria-hidden":"true"}},[e._v("#")]),e._v(" Autohelm Workflow")]),e._v(" "),a("p",[e._v("In order to assist the actual deployment of autohelm files, a docker image has been created with all the neccessary libraries installed, for details how to use this Docker image to aid deployment see this "),a("router-link",{attrs:{to:"/autohelm-workflow.html"}},[e._v("Autohelm Workflow")])],1),e._v(" "),a("h2",{attrs:{id:"telepresence"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#telepresence","aria-hidden":"true"}},[e._v("#")]),e._v(" Telepresence")]),e._v(" "),a("p",[a("a",{attrs:{href:"https://www.telepresence.io/reference/install",target:"_blank",rel:"noopener noreferrer"}},[e._v("Installing Telepresence"),a("OutboundLink")],1)]),e._v(" "),a("p",[e._v("Telepresence is a powerful tool that allows us to create a VPN connection to the cluster Kubectl is connected to. It also allows us to replace running containers in the cluster with docker images running on our local machines.")]),e._v(" "),a("h2",{attrs:{id:"supported-platforms"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#supported-platforms","aria-hidden":"true"}},[e._v("#")]),e._v(" Supported Platforms")]),e._v(" "),a("p",[e._v("The following guide apllies to Mac and Linux (Ubuntu), when running in "),a("code",[e._v("vpn-tcp")]),e._v(" mode (the default), Telepresence will allow all processes running on the machine running Telepresence to access any workloads on the target cluster directly. Running a Telepresence VPN allows local development of components requiring connectivity to one or more other services without needing those service running on the local workstation.")]),e._v(" "),a("p",[e._v("On Windows currently only one telepresence connection method is supported ("),a("code",[e._v("inject-tcp")]),e._v("), this allows cluster connectivity from a single process running on the machine running Telepresence.\nWindows users needing VPN connectivity to clusters beyond a single terminal process are advised to go with VM based solution using the Ubuntu Platform.")]),e._v(" "),a("p",[e._v("See this link for more information:")]),e._v(" "),a("p",[a("a",{attrs:{href:"https://www.telepresence.io/reference/windows",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://www.telepresence.io/reference/windows"),a("OutboundLink")],1)]),e._v(" "),a("h2",{attrs:{id:"prequisites"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#prequisites","aria-hidden":"true"}},[e._v("#")]),e._v(" Prequisites")]),e._v(" "),a("ul",[a("li",[e._v("Supported Platform")]),e._v(" "),a("li",[e._v("kubectl")]),e._v(" "),a("li",[e._v("kubeconfig configured with appropriate context")])]),e._v(" "),a("h2",{attrs:{id:"installation"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#installation","aria-hidden":"true"}},[e._v("#")]),e._v(" Installation")]),e._v(" "),a("p",[e._v("Ensure you have all Prequisites installed, then run command below:")]),e._v(" "),a("div",{staticClass:"language-bash extra-class"},[a("pre",{pre:!0,attrs:{class:"language-bash"}},[a("code",[a("span",{pre:!0,attrs:{class:"token function"}},[e._v("curl")]),e._v(" -s https://packagecloud.io/install/repositories/datawireio/telepresence/script.deb.sh "),a("span",{pre:!0,attrs:{class:"token operator"}},[e._v("|")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[e._v("sudo")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[e._v("bash")]),e._v("\n"),a("span",{pre:!0,attrs:{class:"token function"}},[e._v("sudo")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[e._v("apt")]),e._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[e._v("install")]),e._v(" --no-install-recommends telepresence\n")])])]),a("p",[e._v("For detailed instructions follow installation guide here:")]),e._v(" "),a("p",[a("a",{attrs:{href:"https://www.telepresence.io/reference/install",target:"_blank",rel:"noopener noreferrer"}},[e._v("https://www.telepresence.io/reference/install"),a("OutboundLink")],1)]),e._v(" "),a("h2",{attrs:{id:"usage"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#usage","aria-hidden":"true"}},[e._v("#")]),e._v(" Usage")]),e._v(" "),a("div",{staticClass:"language- extra-class"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[e._v("shell> telepresence\nT: Starting proxy with method 'vpn-tcp', which has the following limitations: All processes are affected, only one telepresence can run per machine, and you can't use other VPNs. You may need to add cloud hosts and headless services with --also-proxy. For a full list of method\nT:  limitations see https://telepresence.io/reference/methods.html\nT: Volumes are rooted at $TELEPRESENCE_ROOT. See https://telepresence.io/howto/volumes.html for details.\n\nT: No traffic is being forwarded from the remote Deployment to your local machine. You can use the --expose option to specify which ports you want to forward.\n\nT: Guessing that Services IP range is 10.43.0.0/16. Services started after this point will be inaccessible if are outside this range; restart telepresence if you can't access a new Service.\n@Default|bash-3.2$\n")])])]),a("p",[e._v("This has opened a VPN connection and you should be able to access kubernetes services using the kubernetes DNS endpoints.")]),e._v(" "),a("p",[e._v("For example")]),e._v(" "),a("p",[e._v("http://activity-log.stg-tennant.svc.cluster.local")]),e._v(" "),a("p",[e._v("The hostnames always follow the pattern {SERVICE_NAME}.{NAMESPACE}.svc.cluster.local")])])}),[],!1,null,null,null);t.default=r.exports}}]);